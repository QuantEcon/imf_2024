\begin{Verbatim}[commandchars=\\\{\},codes={\catcode`\$=3\catcode`\^=7\catcode`\_=8\relax}]
\PYG{k+kn}{import} \PYG{n+nn}{jax.numpy} \PYG{k}{as} \PYG{n+nn}{jnp}
\PYG{k+kn}{from} \PYG{n+nn}{jax} \PYG{k+kn}{import} \PYG{n}{grad}\PYG{p}{,} \PYG{n}{jit}

\PYG{k}{def} \PYG{n+nf}{predict}\PYG{p}{(}\PYG{n}{params}\PYG{p}{,} \PYG{n}{x}\PYG{p}{):}
  \PYG{k}{for} \PYG{n}{W}\PYG{p}{,} \PYG{n}{b} \PYG{o+ow}{in} \PYG{n}{params}\PYG{p}{:}
    \PYG{n}{y} \PYG{o}{=} \PYG{n}{jnp}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{W}\PYG{p}{,} \PYG{n}{x}\PYG{p}{)} \PYG{o}{+} \PYG{n}{b}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{jnp}\PYG{o}{.}\PYG{n}{tanh}\PYG{p}{(}\PYG{n}{y}\PYG{p}{)}
  \PYG{k}{return} \PYG{n}{y}

\PYG{k}{def} \PYG{n+nf}{loss}\PYG{p}{(}\PYG{n}{params}\PYG{p}{,} \PYG{n}{x}\PYG{p}{,} \PYG{n}{targets}\PYG{p}{):}
  \PYG{n}{preds} \PYG{o}{=} \PYG{n}{predict}\PYG{p}{(}\PYG{n}{params}\PYG{p}{,} \PYG{n}{x}\PYG{p}{)}
  \PYG{k}{return} \PYG{n}{jnp}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{((}\PYG{n}{preds} \PYG{o}{\PYGZhy{}} \PYG{n}{targets}\PYG{p}{)}\PYG{o}{**}\PYG{l+m+mi}{2}\PYG{p}{)}

\PYG{n}{grad\PYGZus{}loss} \PYG{o}{=} \PYG{n}{jit}\PYG{p}{(}\PYG{n}{grad}\PYG{p}{(}\PYG{n}{loss}\PYG{p}{))}
\PYG{c+c1}{\PYGZsh{} Now use gradient descent on the loss function}
\end{Verbatim}
