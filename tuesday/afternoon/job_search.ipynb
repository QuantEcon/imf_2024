{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "582e8105",
   "metadata": {},
   "source": [
    "# Job Search\n",
    "\n",
    "Uncomment if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d0ea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install quantecon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ffe1b2",
   "metadata": {},
   "source": [
    "In this lecture we study a basic infinite-horizon job search with Markov wage\n",
    "draws \n",
    "\n",
    "The exercise at the end asks you to add recursive preferences and compare\n",
    "the result.\n",
    "\n",
    "We use the following imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2eb7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import quantecon as qe\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from collections import namedtuple\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d39232",
   "metadata": {},
   "source": [
    "## Solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6baad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "\n",
    "default_tolerance = 1e-9\n",
    "default_max_iter = int(1e6)\n",
    "\n",
    "def successive_approx(f,\n",
    "                      x_init,\n",
    "                      tol=default_tolerance,\n",
    "                      max_iter=default_max_iter,\n",
    "                      verbose=True,\n",
    "                      print_skip=1000):\n",
    "\n",
    "    \"Uses successive approximation on f.\"\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Beginning iteration\\n\\n\")\n",
    "\n",
    "    current_iter = 0\n",
    "    x = x_init\n",
    "    error = tol + 1\n",
    "    while error > tol and current_iter < max_iter:\n",
    "        x_new = f(x)\n",
    "        error = jnp.max(jnp.abs(x_new - x))\n",
    "        if verbose and current_iter % print_skip == 0:\n",
    "            print(\"iter = {}, error = {}\".format(current_iter, error))\n",
    "        current_iter += 1\n",
    "        x = x_new\n",
    "\n",
    "    if current_iter == max_iter:\n",
    "        print(f\"Warning: Hit maximum iteration number {max_iter}\")\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(f\"Iteration converged after {current_iter} iterations\")\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def newton_solver(f, \n",
    "                  x_init, \n",
    "                  tol=default_tolerance, \n",
    "                  max_iter=default_max_iter,\n",
    "                  bicgstab_atol=1e-6,\n",
    "                  verbose=True,\n",
    "                  print_skip=1):\n",
    "    \"\"\"\n",
    "    Apply Newton's algorithm to find a fixed point of f. The routine defines \n",
    "    g via g(x) = f(x) - x and then searches for a root of g via Newton's\n",
    "    method, which iterates on \n",
    "\n",
    "        x_{n+1} = x_n - J(x_n)^{-1} g(x_n)\n",
    "\n",
    "    until convergence, where J(x) is the Jacobian of g at x. The implementation \n",
    "    below defines \n",
    "\n",
    "        q(x) := x - J(x)^{-1} g(x)\n",
    "\n",
    "    and passes this function to successive_approx.\n",
    "\n",
    "    To compute J(x)^{-1} g(x) we can in principle use\n",
    "    `jnp.linalg.solve(jax.jacobian(g)(x), g(x))`. However, this operation is\n",
    "    very memory intensive when x is high-dimensional. It also requires that g\n",
    "    is a regular 2D array (matrix), which necessitates conversion to a single\n",
    "    index. \n",
    "\n",
    "    To avoid instantiating the large matrix J(x), we use jax.jvp to define the\n",
    "    linear map v -> J(x) v. This map is computed on demand for any given v,\n",
    "    which avoids instantiating J(x).  We then pass this to a solver that can\n",
    "    invert arbitrary linear maps.\n",
    "    \"\"\"\n",
    "    g = lambda x: f(x) - x\n",
    "    @jax.jit\n",
    "    def q(x):\n",
    "        # First we define the map v -> J(x) v from x and g\n",
    "        jac_x_prod = lambda v: jax.jvp(g, (x,), (v,))[1]\n",
    "        # Next we compute J(x)^{-1} g(x).  Currently we use \n",
    "        # sparse.linalg.bicgstab. Another option is sparse.linalg.bc\n",
    "        # but this operation seems to be less stable.\n",
    "        b = jax.scipy.sparse.linalg.bicgstab(\n",
    "                jac_x_prod, g(x), \n",
    "                atol=bicgstab_atol)[0]\n",
    "        return x - b\n",
    "    return successive_approx(q, x_init, tol, max_iter, verbose, print_skip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fea3fb",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We study an elementary model where \n",
    "\n",
    "* jobs are permanent \n",
    "* unemployed workers receive current compensation $c$\n",
    "* the wage offer distribution $\\{W_t\\}$ is Markovian\n",
    "* the horizon is infinite\n",
    "* an unemployment agent discounts the future via discount factor $\\beta \\in (0,1)$\n",
    "\n",
    "The wage process obeys\n",
    "\n",
    "$$\n",
    "    W_{t+1} = \\rho W_t + \\nu Z_{t+1},\n",
    "    \\qquad \\{Z_t\\} \\text{ is IID and } N(0, 1)\n",
    "$$\n",
    "\n",
    "We discretize this using Tauchen's method to produce a stochastic matrix $P$\n",
    "\n",
    "Since jobs are permanent, the return to accepting wage offer $w$ today is\n",
    "\n",
    "$$\n",
    "    w + \\beta w + \\beta^2 w + \\frac{w}{1-\\beta}\n",
    "$$\n",
    "\n",
    "The Bellman equation is\n",
    "\n",
    "$$\n",
    "    v(w) = \\max\n",
    "    \\left\\{\n",
    "            \\frac{w}{1-\\beta}, c + \\beta \\sum_{w'} v(w') P(w, w')\n",
    "    \\right\\}\n",
    "$$\n",
    "\n",
    "We solve this model using value function iteration.\n",
    "\n",
    "\n",
    "## Code\n",
    "\n",
    "Let's set up a namedtuple to store information needed to solve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bf932f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = namedtuple('Model', ('n', 'w_vals', 'P', 'β', 'c', 'θ'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6f0ecb",
   "metadata": {},
   "source": [
    "The function below holds default values and populates the namedtuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc45fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_js_model(\n",
    "        n=500,       # wage grid size\n",
    "        ρ=0.9,       # wage persistence\n",
    "        ν=0.2,       # wage volatility\n",
    "        β=0.99,      # discount factor\n",
    "        c=1.0,       # unemployment compensation\n",
    "        θ=-0.1       # risk parameter\n",
    "    ):\n",
    "    \"Creates an instance of the job search model with Markov wages.\"\n",
    "    mc = qe.tauchen(n, ρ, ν)\n",
    "    w_vals, P = jnp.exp(mc.state_values), mc.P\n",
    "    P = jnp.array(P)\n",
    "    return Model(n, w_vals, P, β, c, θ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c6c94f",
   "metadata": {},
   "source": [
    "Here's the Bellman operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e6fa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def T(v, model):\n",
    "    \"\"\"\n",
    "    The Bellman operator Tv = max{e, c + β E v} with \n",
    "\n",
    "        e(w) = w / (1-β) and (Ev)(w) = E_w[ v(W')]\n",
    "\n",
    "    \"\"\"\n",
    "    n, w_vals, P, β, c, θ = model\n",
    "    h = c + β * P @ v\n",
    "    e = w_vals / (1 - β)\n",
    "\n",
    "    return jnp.maximum(e, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbf098f",
   "metadata": {},
   "source": [
    "The next function computes the optimal policy under the assumption that $v$ is\n",
    "                 the value function.\n",
    "\n",
    "The policy takes the form\n",
    "\n",
    "$$\n",
    "    \\sigma(w) = \\mathbf 1 \n",
    "        \\left\\{\n",
    "            \\frac{w}{1-\\beta} \\geq c + \\beta \\sum_{w'} v(w') P(w, w')\n",
    "        \\right\\}\n",
    "$$\n",
    "\n",
    "Here $\\mathbf 1$ is an indicator function.\n",
    "\n",
    "The statement above means that the worker accepts ($\\sigma(w) = 1$) when the value of stopping\n",
    "is higher than the value of continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b18a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def get_greedy(v, model):\n",
    "    \" Get a v-greedy policy.\"\n",
    "    n, w_vals, P, β, c, θ = model\n",
    "    e = w_vals / (1 - β)\n",
    "    h = c + β * P @ v\n",
    "    σ = jnp.where(e >= h, 1, 0)\n",
    "    return σ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a96b157",
   "metadata": {},
   "source": [
    "Here's a routine for value function iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9b7b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vfi(model, max_iter=10_000, tol=1e-4):\n",
    "    \"Solve the infinite-horizon Markov job search model by VFI.\"\n",
    "    print(\"Starting VFI iteration.\")\n",
    "    v = jnp.zeros_like(model.w_vals)    # Initial guess\n",
    "    i = 0\n",
    "    error = tol + 1\n",
    "\n",
    "    while error > tol and i < max_iter:\n",
    "        new_v = T(v, model)\n",
    "        error = jnp.max(jnp.abs(new_v - v))\n",
    "        i += 1\n",
    "        v = new_v\n",
    "\n",
    "    v_star = v\n",
    "    σ_star = get_greedy(v_star, model)\n",
    "    return v_star, σ_star"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5a768b",
   "metadata": {},
   "source": [
    "## Computing the solution\n",
    "\n",
    "Let's set up and solve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a67b23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_js_model()\n",
    "n, w_vals, P, β, c, θ = model\n",
    "\n",
    "qe.tic()\n",
    "v_star, σ_star = vfi(model)\n",
    "vfi_time = qe.toc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193b5af4",
   "metadata": {},
   "source": [
    "We compute the reservation wage as the first $w$ such that $\\sigma(w)=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c007ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_wage = w_vals[jnp.searchsorted(σ_star, 1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7c0528",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(w_vals, v_star, alpha=0.8, label=\"value function\")\n",
    "ax.vlines((res_wage,), 150, 400, 'k', ls='--', label=\"reservation wage\")\n",
    "ax.legend(frameon=False, fontsize=12, loc=\"lower right\")\n",
    "ax.set_xlabel(\"$w$\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c32fd5",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "In the setting above, the agent is risk-neutral vis-a-vis future utility risk.\n",
    "\n",
    "Now solve the same problem but this time assuming that the agent has risk-sensitive\n",
    "preferences, which are a type of nonlinear recursive preferences.\n",
    "\n",
    "The Bellman equation becomes\n",
    "\n",
    "$$\n",
    "    v(w) = \\max\n",
    "    \\left\\{\n",
    "            \\frac{w}{1-\\beta}, \n",
    "            c + \\frac{\\beta}{\\theta}\n",
    "            \\ln \\left[ \n",
    "                      \\sum_{w'} \\exp(\\theta v(w')) P(w, w')\n",
    "                \\right]\n",
    "    \\right\\}\n",
    "$$\n",
    "\n",
    "\n",
    "When $\\theta < 0$ the agent is risk sensitive.\n",
    "\n",
    "Solve the model when $\\theta = -0.1$ and compare your result to the risk neutral\n",
    "case.\n",
    "\n",
    "Try to interpret your result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73273243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_risk_sensitive_js_model(\n",
    "        n=500,       # wage grid size\n",
    "        ρ=0.9,       # wage persistence\n",
    "        ν=0.2,       # wage volatility\n",
    "        β=0.99,      # discount factor\n",
    "        c=1.0,       # unemployment compensation\n",
    "        θ=-0.1       # risk parameter\n",
    "    ):\n",
    "    \"Creates an instance of the job search model with Markov wages.\"\n",
    "    mc = qe.tauchen(n, ρ, ν)\n",
    "    w_vals, P = jnp.exp(mc.state_values), mc.P\n",
    "    P = jnp.array(P)\n",
    "    return Model(n, w_vals, P, β, c, θ)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def T_rs(v, model):\n",
    "    \"\"\"\n",
    "    The Bellman operator Tv = max{e, c + β R v} with \n",
    "\n",
    "        e(w) = w / (1-β) and\n",
    "\n",
    "        (Rv)(w) = (1/θ) ln{E_w[ exp(θ v(W'))]}\n",
    "\n",
    "    \"\"\"\n",
    "    n, w_vals, P, β, c, θ = model\n",
    "    h = c + (β / θ) * jnp.log(P @ (jnp.exp(θ * v)))\n",
    "    e = w_vals / (1 - β)\n",
    "\n",
    "    return jnp.maximum(e, h)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def get_greedy_rs(v, model):\n",
    "    \" Get a v-greedy policy.\"\n",
    "    n, w_vals, P, β, c, θ = model\n",
    "    e = w_vals / (1 - β)\n",
    "    h = c + (β / θ) * jnp.log(P @ (jnp.exp(θ * v)))\n",
    "    σ = jnp.where(e >= h, 1, 0)\n",
    "    return σ\n",
    "\n",
    "\n",
    "\n",
    "def vfi(model, max_iter=10_000, tol=1e-4):\n",
    "    \"Solve the infinite-horizon Markov job search model by VFI.\"\n",
    "    print(\"Starting VFI iteration.\")\n",
    "    v = jnp.zeros_like(model.w_vals)    # Initial guess\n",
    "    i = 0\n",
    "    error = tol + 1\n",
    "\n",
    "    while error > tol and i < max_iter:\n",
    "        new_v = T_rs(v, model)\n",
    "        error = jnp.max(jnp.abs(new_v - v))\n",
    "        i += 1\n",
    "        v = new_v\n",
    "\n",
    "    v_star = v\n",
    "    σ_star = get_greedy_rs(v_star, model)\n",
    "    return v_star, σ_star\n",
    "\n",
    "\n",
    "\n",
    "model_rs = create_risk_sensitive_js_model()\n",
    "\n",
    "n, w_vals, P, β, c, θ = model_rs\n",
    "\n",
    "qe.tic()\n",
    "v_star_rs, σ_star_rs = vfi(model_rs)\n",
    "vfi_time = qe.toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9d7a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_wage_rs = w_vals[jnp.searchsorted(σ_star_rs, 1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0327b109",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(w_vals, v_star,  alpha=0.8, label=\"RN $v$\")\n",
    "ax.plot(w_vals, v_star_rs, alpha=0.8, label=\"RS $v$\")\n",
    "ax.vlines((res_wage,), 150, 400,  ls='--', color='darkblue', alpha=0.5, label=r\"RV $\\bar w$\")\n",
    "ax.vlines((res_wage_rs,), 150, 400, ls='--', color='orange', alpha=0.5, label=r\"RS $\\bar w$\")\n",
    "ax.legend(frameon=False, fontsize=12, loc=\"lower right\")\n",
    "ax.set_xlabel(\"$w$\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013f74f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
